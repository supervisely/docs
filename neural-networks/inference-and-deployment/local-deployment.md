## (🚧 Work in progress)

## Deploy in Docker

You can deploy a model in a 🐋 Docker Container with a single `docker run` command. 
Once the model deployed, it starts FastAPI Server, being ready to process API requests for inference. It allows you to predict with local images, folders, videos, or remote supervisely projects and datasets (you need to provide your Supervisely API token for that).
